{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextProbe Quickstart Guide\n",
    "\n",
    "TextProbe is an **all-in-one** Text Analysis API.  The API is organized into four core endpoints which return a rich analysis of your text across many dimensions:\n",
    "1. **Entities Endpoint**:  [extracts entities](https://en.wikipedia.org/wiki/Named-entity_recognition) like people, groups, and places with automatic [entity linking](https://en.wikipedia.org/wiki/Entity_linking) to the [Wikidata](https://www.wikidata.org/wiki/Wikidata:Main_Page) knowledge base. Also extracts [measurements](https://en.wikipedia.org/wiki/Measured_quantity#:~:text=In%20a%20physical%20setting%20a,the%20context%20of%20quantum%20mechanics) (e.g., *10 cm* from \"pencil is 10cm\") and measured things (e.g., *pencil* from \"pencil is 10cm\").\n",
    "2. **Relations Endpoint**: [extracts relations](https://en.wikipedia.org/wiki/Relationship_extraction) between entities including certain causal relatonships.  Also extracts quotations (speaker->said->quote).\n",
    "3. **Topics Endpoint**: [auto-categorizes](https://en.wikipedia.org/wiki/Document_classification) text by subject matter. Also extracts salient concepts or subthemes as represented by [keyphrases](https://en.wikipedia.org/wiki/Keyword_extraction).\n",
    "4. **Feelings Endpoint**: detects [emotion](https://en.wikipedia.org/wiki/Emotion_recognition) (e.g., joy, anger, fear, sadness, neutral) and [sentiment](https://en.wikipedia.org/wiki/Sentiment_analysis) (i.e., positive vs. negative vs. neutral) including [entity sentiment](https://www.semanticscholar.org/paper/Entity-Based-Sentiment-Analysis-on-Twitter-Batra-Rao/b6595c1bb5ae1bd6ea1fd9bf561796dd84c25295) \n",
    "\n",
    "\n",
    "TextProbe is a REST API that accepts JSON requests via both POST and GET.  Thus, you can access our API with any programming language (e.g., Python, PHP, Node.js, C#) and with very little coding required. We will use Python in this notebook and send requests using the `requests` library. Let's begin by installing `requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests # install requests if not already installed\n",
    "import requests          # import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Getting an API Key\n",
    "\n",
    "TextProbe currently offers our API through [RapidAPI](https://rapidapi.com/marketplace).  A single API request can be used to perform all analyses listed above.  Our API can be accessed for free for up to 500 requests per day with no credit card required. \n",
    "\n",
    "To get started, you can:\n",
    "1. [Register an account](https://rapidapi.com/marketplace) with RapidAPI for free\n",
    "2. [Subscribe](https://rapidapi.com/textprobe/api/textprobe/pricing) to the TextProbe API.  Choose the Basic Plan to try it out for free.\n",
    "2. [Go here](https://rapidapi.com/textprobe/api/textprobe/endpoints) and make note of your API key. It will be the value associated with the `x-rapidapi-key` field on the lower right of the screen under **Code Snippets**.\n",
    "\n",
    "The RapidAPI site also shows example code for accessing the API through other languages and libraries such as Node.js.\n",
    "\n",
    "Once you obtain a free API key, enter it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'ENTER YOUR API KEY HERE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2:  Define Request Function\n",
    "\n",
    "Let's define a convience function to help send our requests to different TextProbe endpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://textprobe.p.rapidapi.com'\n",
    "def post_request(doc, endpoint, **kwargs):\n",
    "    kwargs['text'] = doc\n",
    "    r = requests.post(url = BASE_URL+endpoint, json = kwargs, \n",
    "                      headers={'x-rapidapi-key': API_KEY})\n",
    "    print('status code: %s' % (r.status_code))\n",
    "    if r.status_code in range(500,599): return {\"detail\" : \"Internal Server Error\"}\n",
    "    data = r.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3:   Analyze Your Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the text we will analyze in this notebook, which consists of two paragraphs (i.e., two sets of sentences separated with blank line).  When sending text to TextProbe, we recommend you try to retain this kind of paragraph structure if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"\n",
    "Artifical intelligence remains a hot area in the technology sector.\n",
    "In 2014, Google bought London-based artificial intelligence company DeepMind with an acquisition\n",
    "price was more than $500 million. Facebook was also in talks to buy the startup in late 2013.\n",
    "DeepMind had confirmed the acquisition, but couldnâ€™t disclose deal terms.\n",
    "CMU Professor Larry Wasserman once wrote that \"the startup is trying to build a system that thinks.\"\n",
    "\n",
    "As tech giants increase their AI acquisitions, some are concerned about the harmful effects of AI\n",
    "and the role of Big Tech in society in general. Indeed, many we interviewed now feel that Facebook is \n",
    "\"oficially evil\" due to policies that force the sharing of personal data among other transgressions.  \n",
    "Many are critical of Apple for other reasons, but happily continue to buy their iPhones and MacBooks \n",
    "with 3.2GHz M1 processors.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now send the above text to each of our four endpoints for analysis.  Let's begin with the **Entities** endpoint.\n",
    "\n",
    "### Entities Endpoint\n",
    "\n",
    "Let's now send our text to the **/entities** endpoint using our `post_request` convenience function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code: 200\n"
     ]
    }
   ],
   "source": [
    "result = post_request(document, '/entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **/entities** endpoint returns a dictionary with three main keys.\n",
    "1. `sentences`: the original text split into sentences with slight normalizations to facilitate analyses\n",
    "2. `entities`: extracted entities like people, groups, and places\n",
    "3. `measurements`: extracted measured quantities along with properties and attributes being measured\n",
    "\n",
    "In addition, all endpoints will return a `version` and `message` field, which return the API version and any messages from the API, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentences', 'entities', 'measurements', 'version', 'message'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through each of the three main keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sentences`\n",
    "\n",
    "The value for `sentences` is the original text split into sentences with slight normalizations necessary for analysis.  For example, you'll notice that **3.2GHz** in the last sentence was normalized to **3.2 GHZ** (with a space separating the quantity and measurement unit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artifical intelligence remains a hot area in the technology sector.',\n",
       " 'In 2014, Google bought London-based artificial intelligence company DeepMind with an acquisition price was more than $500 million.',\n",
       " 'Facebook was also in talks to buy the startup in late 2013.',\n",
       " \"DeepMind had confirmed the acquisition, but couldn't disclose deal terms.\",\n",
       " 'CMU Professor Larry Wasserman once wrote that \"the startup is trying to build a system that thinks.\"',\n",
       " 'As tech giants increase their AI acquisitions, some are concerned about the harmful effects of AI and the role of Big Tech in society in general.',\n",
       " 'Indeed, many we interviewed now feel that Facebook is  \"oficially evil\" due to policies that force the sharing of personal data among other transgressions.',\n",
       " 'Many are critical of Apple for other reasons, but happily continue to buy their iPhones and MacBooks  with 3.2 GHz M1 processors.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `entities`\n",
    "\n",
    "The `entities` field of the response is a list of extracted entities (e.g., people, groups, places).  The `entity_type` fields show the categorization of each entity (e.g., **Person**, **Group**, **Place**, **Product**).  The `sentence_id` shows from which sentence the entity was extracted, and the `start` and  `end` fields show the character span of the entity within that sentence.  Entities are automatically disambiguated and linked to the [Wikidata Knowledgebase](https://www.wikidata.org) when possible.  For instance, the first entity, **Google**, is linked to entry [Q95](https://www.wikidata.org/wiki/Q95) in the Wikidata knowledgebase, which can be accessed to retrieve more information on the company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'Google',\n",
       "  'start': 9,\n",
       "  'end': 15,\n",
       "  'entity_type': 'Group',\n",
       "  'wikidata_id': 'Q95',\n",
       "  'wikidata_name': 'Google',\n",
       "  'sentence_id': 1},\n",
       " {'entity': 'London',\n",
       "  'entity_type': 'Place',\n",
       "  'start': 23,\n",
       "  'end': 29,\n",
       "  'sentence_id': 1},\n",
       " {'entity': 'DeepMind',\n",
       "  'start': 68,\n",
       "  'end': 76,\n",
       "  'entity_type': 'Group',\n",
       "  'wikidata_id': 'Q15733006',\n",
       "  'wikidata_name': 'DeepMind',\n",
       "  'sentence_id': 1},\n",
       " {'entity': 'more than $500 million',\n",
       "  'entity_type': 'Money',\n",
       "  'start': 107,\n",
       "  'end': 129,\n",
       "  'sentence_id': 1},\n",
       " {'entity': 'DeepMind',\n",
       "  'start': 0,\n",
       "  'end': 8,\n",
       "  'entity_type': 'Group',\n",
       "  'wikidata_id': 'Q15733006',\n",
       "  'wikidata_name': 'DeepMind',\n",
       "  'sentence_id': 3},\n",
       " {'entity': 'CMU',\n",
       "  'entity_type': 'Group',\n",
       "  'start': 0,\n",
       "  'end': 3,\n",
       "  'sentence_id': 4},\n",
       " {'entity': 'Larry Wasserman',\n",
       "  'start': 14,\n",
       "  'end': 29,\n",
       "  'entity_type': 'Person',\n",
       "  'wikidata_id': 'Q6489856',\n",
       "  'wikidata_name': 'Larry A. Wasserman',\n",
       "  'sentence_id': 4},\n",
       " {'entity': 'AI',\n",
       "  'entity_type': 'Group',\n",
       "  'start': 30,\n",
       "  'end': 32,\n",
       "  'sentence_id': 5},\n",
       " {'entity': 'AI',\n",
       "  'entity_type': 'Group',\n",
       "  'start': 95,\n",
       "  'end': 97,\n",
       "  'sentence_id': 5},\n",
       " {'entity': 'Big Tech',\n",
       "  'start': 114,\n",
       "  'end': 122,\n",
       "  'entity_type': 'Group',\n",
       "  'wikidata_id': 'Q65040888',\n",
       "  'wikidata_name': 'Big Five',\n",
       "  'sentence_id': 5},\n",
       " {'entity': 'Apple',\n",
       "  'start': 21,\n",
       "  'end': 26,\n",
       "  'entity_type': 'Group',\n",
       "  'wikidata_id': 'Q312',\n",
       "  'wikidata_name': 'Apple Inc.',\n",
       "  'sentence_id': 7},\n",
       " {'entity': 'iPhone',\n",
       "  'start': 80,\n",
       "  'end': 87,\n",
       "  'entity_type': 'Product',\n",
       "  'wikidata_id': 'Q2766',\n",
       "  'wikidata_name': 'iPhone',\n",
       "  'sentence_id': 7},\n",
       " {'entity': 'MacBook',\n",
       "  'start': 92,\n",
       "  'end': 100,\n",
       "  'entity_type': 'Product',\n",
       "  'wikidata_id': 'Q3065317',\n",
       "  'wikidata_name': 'MacBook',\n",
       "  'sentence_id': 7}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['entities']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **/entities** endpoint accepts the optional argument, `enable_freebase`.  If True, a richer, pipe-delimited list of freebase types will be assigned to entities where possible.  For instance, here are the freebase types for Google:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code: 200\n",
      "{'entity': 'Google', 'start': 9, 'end': 15, 'entity_type': 'Group', 'wikidata_id': 'Q95', 'wikidata_name': 'Google', 'freebase_types': '/organization/organization_member | /influence/influence_node | /business/consumer_company | /computer/operating_system_developer | /conferences/conference_sponsor | /internet/website_owner | /venture_capital/venture_funded_company | /business/brand | /dataworld/data_provider | /computer/computer_manufacturer_brand | /education/educational_institution | /travel/hotel_grading_authority | /freebase/list | /venture_capital/venture_investor | /business/business_operation | /law/litigant | /organization/organization_founder | /freebase/freebase_interest_group | /book/book_subject | /computer/software_developer | /award/ranked_item | /organization/organization | /business/sponsor | /award/award_presenting_organization | /award/award_winner | /business/customer | /organization/organization_partnership | /business/issuer | /law/patent_assignee | /film/film_subject | /conferences/conference_subject | /computer/programming_language_developer | /award/award_nominee | /architecture/architectural_structure_owner | /dataworld/information_source | /projects/project_participant | /business/employer | /book/book | /book/written_work', 'sentence_id': 1}\n"
     ]
    }
   ],
   "source": [
    "result = post_request(document, '/entities', enable_freebase=True)\n",
    "print(result['entities'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `measurements`\n",
    "\n",
    "The `measurements` field of the response will contain a list of the extracted measured quantities.  Our measurement extractor can discover measurements even when corrupted through PDF-to-text conversions (e.g., lost exponents and distorted scientific notation). \n",
    "\n",
    "In addition to containing extracted measured quantities, each returned dictionary may also include an extra entry `thing_measured` which will contain, the object, property or attribute being measured (when possible). In this case, we see that it is the **M1 processor** that is being measured (at **3.2 GHz**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'measurement': '3.2 GHz',\n",
       "  'units': 'GHz',\n",
       "  'number': 3.2,\n",
       "  'start': 107,\n",
       "  'end': 114,\n",
       "  'sentence_id': 7,\n",
       "  'thing_measured': 'M1 processors'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['measurements']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relations Endpoint\n",
    "\n",
    "The **/relations** endpoint extracts relations among entities from the text supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code: 200\n"
     ]
    }
   ],
   "source": [
    "result = post_request(document, '/relations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from **/relations** contains two main keys: `relations` and `quotations` (in addition to `version` and `message`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['relations', 'quotations', 'version', 'message'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `relations`\n",
    "The `relations` field contains a list of discovered relations from the supplied text. In this case, we have discovered and extracted Merger/Acquisition activity (i.e., Google acquires DeepMind) that can be useful for a variety of downstream tasks.  The `sentence` from where the relations was extracted is also included.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'subject': 'Google',\n",
       "  'predicate': 'bought',\n",
       "  'object': 'London - based artificial intelligence company DeepMind',\n",
       "  'subject_type': 'Group',\n",
       "  'object_type': 'Group',\n",
       "  '_sentence': 'In 2014, Google bought London-based artificial intelligence company DeepMind with an acquisition price was more than $500 million.',\n",
       "  'subject_id': 'Q95',\n",
       "  'object_id': 'Q15733006'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['relations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `quotations`\n",
    "The `quotations` entry in the response will include a list of extracted quotations. (Quotations can be viewed as a kind of relation between a speaker and a statement.  Each quotation entry includes a `polarity` field which will indicate the positive (closer to 1.0) or negative (closer to -1.0) sentiment of the quote.  In this case, the quotation is neutral (i.e., `polarity=0.0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'speaker': 'Professor Larry Wasserman',\n",
       "  'action': 'wrote',\n",
       "  'quote': '\"the startup is trying to build a system that thinks.\"',\n",
       "  'polarity': 0.0,\n",
       "  'subject_id': 'Q6489856'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['quotations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the examples above, TextProbe will automatically link entities discovered in the subject and object of a relation to the Wikidata knowledgebase.  For instance, the speaker of the above quote, Larry Wasserman, is linked to the appropriate Wikidata entry ([Q6489856](https://www.wikidata.org/wiki/Q6489856)).  \n",
    "\n",
    "In addition, although not illustrated in this example, the TextProbe can also optionally perform [coreference resolution](https://en.wikipedia.org/wiki/Coreference) over the first ten sentences in the text. That is, words like \"he\", \"she\", and \"they\" are replaced with the entities that they represent. To turn this behavior off, you can supply `enable_coref=False` to the **/relations** endpoint. We will show an example of this below in the **Tips and Tricks** section.\n",
    "\n",
    "### Topics Endpoint\n",
    "\n",
    "The **/topics** endpoint extracts information related to the underlying subject matter of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code: 200\n"
     ]
    }
   ],
   "source": [
    "result = post_request(document, '/topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from **/topics** contains two main keys: `categories` and `keywords` (in addition to `version` and `message`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['categories', 'keywords', 'version', 'message'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `categories`\n",
    "The **/topics** endpoint categorizes the text into one or more of about 40 different topic categories.  The `categories` field contains the predicted categories for the text. Here, we see the text is primarily classified into **Tech**, which makes sense for this text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Business/Finance': 0.124656, 'Tech': 0.775994}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['categories']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `keywords`\n",
    "Extracted keyphrases representing important concepts and subthemes in the text are available in the `keywords` field of the response. Here, we can see **artificial intellgence** as the top salient concept in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artifical intelligence',\n",
       " 'AI',\n",
       " 'acquisition',\n",
       " 'Facebook',\n",
       " 'startup',\n",
       " 'Google',\n",
       " 'London',\n",
       " 'CMU',\n",
       " 'Professor']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['keywords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feelings Endpoint\n",
    "\n",
    "The **/feelings** endpoint extracts information related to the underlying emotions and sentiments expressed in the text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code: 200\n"
     ]
    }
   ],
   "source": [
    "result = post_request(document, '/feelings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response from **/feelings** will return up to five main fields (in addition to `message` and `version`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['emotion_prediction', 'emotion_scores', 'sentiment_prediction', 'sentiment_scores', 'entity_sentiments', 'version', 'message'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `sentiment_prediction` and `sentiment_scores`\n",
    "The `sentiment_prediction` contains a string indicating whether the supplied text as a whole is `positive`, `negative`, or `neutral`.  In this case, the overall sentiment of the text is neutral (i.e., only very slightly negative), since the article is mostly making objective statements about AI acquisitions..  The `sentiment_scores` field shows the probability scores for the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral\n",
      "{'Positive': 0.52265625, 'Negative': 0.47734374999999996}\n"
     ]
    }
   ],
   "source": [
    "print(result['sentiment_prediction'])\n",
    "print(result['sentiment_scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `emotion_prediction` and `emotion_scores`\n",
    "\n",
    "TextProbe can also discover the overall emotional tone of the text. The `emotion_prediction` field categorizes the text into one of five emotional categories including a *Neutral* category.  The `emotion_scores` show the predicted probabilities for each emotion.  In this case, due to the concerns about Big Tech expressed in the text, the predicted emotion of the text is **Fear**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fear\n",
      "{'Anger': 0.003734, 'Fear': 0.98739, 'Joy': 0.001806, 'Neutral': 0.002218, 'Sadness': 0.004853}\n"
     ]
    }
   ],
   "source": [
    "print(result['emotion_prediction'])\n",
    "print(result['emotion_scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `entity_sentiment`\n",
    "\n",
    "The **/feelings** endpoint can also detect sentiments towards specific entities, which, when available, are stored in the `entity_sentiments` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'Facebook', 'polarity': -0.6597, 'descriptor': 'evil'}]\n"
     ]
    }
   ],
   "source": [
    "print(result['entity_sentiments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the overall sentiment is neutral, we see here that there is negative sentiment expressed specifically towards Facebook buried in the text, as contained in the `entity_sentiments` field.  As with quotation extraction, the sentiment score is included as `polarity`. Entity sentiments are currently limited to adjetival forms of sentiment where the entity is the subject (e.g., Barack Obama was a great President)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Tricks\n",
    "\n",
    "Here we include some additional tips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting up Larger Texts\n",
    "\n",
    "Each request to TextProbe can be a maximum of 10KB (about 10K ASCII characters).  If your document is larger than 10KB, you can split it up into 10KB chunks and send each chunk as a separate API request. Let's create a document of over 10KB by duplicating the document used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_document = \"\\n\".join([document] * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21799"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(large_document.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 22549 bytes, `large_document` is over the 10KB limit.  We can split into smaller chunks by using the [nltk library](https://pypi.org/project/nltk/) to split the document into sentences.  The function `text2chunks` shown below will split a large text document into chunks of text that are <= 10KB and will also try to prevent paragraphs from being split across chunks whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "def text2chunks(document, limit=10000):\n",
    "    chunks = []\n",
    "    texts = ''\n",
    "    current_length = 0\n",
    "    for paragraph in filter(lambda x : x != '', document.split('\\n\\n')):\n",
    "        sent_list = []\n",
    "        sent_list.extend(sent_tokenize(paragraph))\n",
    "        sentences = ' '.join(sent_list)\n",
    "        sentences = \"\\n\\n%s\" % (sentences)\n",
    "        if len(sentences.encode('utf-8', 'ignore')) > limit:\n",
    "            texts += '\\n\\n'\n",
    "            for sent in sent_list:\n",
    "                if current_length + len(sent.encode('utf-8', 'ignore')) < limit:\n",
    "                    texts += ' %s' % (sent)\n",
    "                    current_length += len(sent.encode('utf-8', 'ignore'))\n",
    "                else:\n",
    "                    chunks.append(texts)\n",
    "                    texts = ''\n",
    "                    current_length = 0                \n",
    "        elif current_length + len(sentences.encode('utf-8', 'ignore')) < limit:\n",
    "            texts += sentences\n",
    "            current_length += len(sentences.encode('utf-8', 'ignore'))\n",
    "\n",
    "        else:\n",
    "            chunks.append(texts)\n",
    "            texts = ''\n",
    "            current_length = 0\n",
    "    if texts: chunks.append(texts)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `text2chunks` to split our large document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text2chunks(large_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of chunks: 3\n",
      "length of chunk 1: 9570 bytes\n",
      "length of chunk 2: 9570 bytes\n",
      "length of chunk 3: 1740 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"# of chunks: %s\" % len(chunks))\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(\"length of chunk %s: %s bytes\" % (i+1, len(chunk.encode('utf-8', 'ignore'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chunk of text can then be sent to TextProbe for analysis as a separate API request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Entities Within Sentence Context\n",
    "An extracted entity (or measurement) can be localized using a combination of the `sentence_id` and the `start` and `end` character indices.  For instance, in the cells below, we will view an entity (Google) and a measurement (3.2 GhZ) both within their respective sentence contexts.  We will highlight the extraction in blue using the `highlight_entity` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_entity(ent, sentence):\n",
    "    def colortext(x):\n",
    "        return '\\033[34m' + str(x) + '\\033[0m'\n",
    "    start = ent['start']\n",
    "    end = ent['end']\n",
    "    print(sentence[:start] + colortext(sentence[start:end]) + sentence[end:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code: 200\n",
      "In 2014, \u001b[34mGoogle\u001b[0m bought London-based artificial intelligence company DeepMind with an acquisition price was more than $500 million.\n"
     ]
    }
   ],
   "source": [
    "result = post_request(document, '/entities')\n",
    "entity = result['entities'][0]\n",
    "sentence_of_entity = result['sentences'][entity['sentence_id']]\n",
    "highlight_entity(entity, sentence_of_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Many are critical of Apple for other reasons, but happily continue to buy their iPhones and MacBooks  with \u001b[34m3.2 GHz\u001b[0m M1 processors.\n"
     ]
    }
   ],
   "source": [
    "entity = result['measurements'][0]\n",
    "sentence_of_entity = result['sentences'][entity['sentence_id']]\n",
    "highlight_entity(entity, sentence_of_entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coreference Resolution\n",
    "\n",
    "As mentioned above, the **/relations** and **/feelings** endpoints accept an optional `enable_coref` argument. When set to `True`, TextProbe will perform [coreference resolution](https://en.wikipedia.org/wiki/Coreference) over the first ten sentences in the supplied text.  Consider the following text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text =  \"\"\"\n",
    "President Obama had a yacht. He loved sailing it. \n",
    "I enjoyed hanging out on his boat because he was a great entertainer. \n",
    "He once told me, \"Sailing is the only thing that   relaxes me.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this text through both the **/relations** endpoint and the **/feelings** endpoint with `enable_coref=True` and observe results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status code: 200\n",
      "status code: 200\n",
      "[{'subject': 'President Obama', 'predicate': 'was', 'object': 'a great entertainer', 'subject_type': 'Person', 'object_type': '', '_sentence': 'I enjoyed hanging out on President Obama boat because President Obama was a great entertainer.', 'subject_id': 'Q76'}]\n",
      "\n",
      "[{'speaker': 'President Obama', 'action': 'told', 'quote': '\"Sailing is the only thing that   relaxes me.\"', 'polarity': 0.0, 'subject_id': 'Q76'}]\n",
      "\n",
      "[{'entity': 'Obama', 'polarity': 0.6249, 'descriptor': 'great'}]\n"
     ]
    }
   ],
   "source": [
    "result_relations = post_request(some_text, '/relations', enable_coref=True)\n",
    "result_feelings = post_request(some_text, '/feelings', enable_coref=True)\n",
    "print(result_relations['relations'])\n",
    "print()\n",
    "print(result_relations['quotations'])\n",
    "print()\n",
    "print(result_feelings['entity_sentiments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the discovered relations, quotations, and entity sentiments related to Obama all involve sentences that only mention \"he\" in the original text. Obama is never explicitly mentioned in these sentences.   With `enable_coref=True`, TextProbe automatically replaces these references allowing for the correct extractions and assignments of these relations.\n",
    "\n",
    "Finally, we note that, when invoking an endpoint for the first time (or with changed parameters like `enable_coref=True`), you might experience a slight initial delay due to the endpoint being \"warmed up\". Subsequent invocations will be faster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Here we have given a quick introduction in how to get up and running with the TextProbe API. You should now be ready to easily explore your own data.\n",
    "\n",
    "If you need support or have questions, please email **support@textprobe.com**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
